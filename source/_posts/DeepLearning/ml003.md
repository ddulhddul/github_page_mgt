---
title: Chapter4. 신경망 학습
date: 2018-01-28 19:23:52
categories:
  - DeepLearning
tags:
  - DeepLearning
  - 밑바닥부터 시작하는 딥러닝
---

![밑바닥부터 시작하는 딥러닝](/images/deeplearning/cover.jpg)

# 4.1 데이터에서 학습한다!
- 가중치 매개변수의 값을 데이터를 보고 자동으로 결정

## 4.1.1 데이터 주도 학습
- 신경망돠 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지님.
- 이미지에서 특징을 추출하고 특징의 패턴을 기계학습 기술로 학습
- 이미지의 특징은 보통 벡터로 기술
- 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계

![Paradigm](/images/deeplearning/chapter4.01paradigm.PNG)
> 회색은 사람이 개입하지 않음.
> 딥러닝을 종단간 기계학습 이라고도 한다. (처음부터 끝까지)

## 4.1.2 훈련 데이터와 시험 데이터
- 기계학습의 문제는 훈련 데이터와 시험 데이터를 나눠 실험을 수행하는 것.
    - 범용적으로 사용할 수 있는 모델을 원하기 때문.
    - 범용 능력을 제대로 평가하기 위해 훈련 데이터와 시험 데이터를 분리.
- 데이터셋 하나에만 지나치게 최적화되어 오버피팅 된 상태가 벌어지기도 한다.

# 4.2 손실 함수
- 신경망 학습에서 현재의 상태를 하나의 지표로 표현한다.
- 신경망 학습에서 사용하는 지표는 **손실 함수** (loss function) 라고 한다.
    - 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용.

> 손실함수는 신경망의 성능의 나쁨을 나타내는 지표이다. 손실 함수에 마이너스를 곱하면 얼마나 좋으냐 라는 지표로 변한다. 성능의 나쁨과 좋음중 어느쪽을 지표로 삼아도 본질적으로 같다.

## 4.2.1 평균 제곱 오차
mean squared error, MSE

![MSE 함수](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)

> Yi : 신경망의 출력(신경망이 추정한 값)
> Y^i : 정답 레이블
> i : 데이터의 차원 수

출처 : [위키백과](https://en.wikipedia.org/wiki/Mean_squared_error)

책에 나온 MSE 함수와 위키에 나온 함수가 다르다 ?

- 평균 제곱 오차를 기준으로 오차가 더 작은 값이 정답에 더 가까운 것.

```python
def mean_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)
```

## 4.2.2 교차 엔트로피 오차
cross entropy error, CEE

![CEE 함수](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)

> q(x) : 신경망의 출력
> p(x) : 정답 레이블

출처 : [위키백과](https://en.wikipedia.org/wiki/Cross_entropy)

- 실질적으로 정답일때의 추정의 자연로그를 계산하는 식이 된다.
- 교차엔트로피 오차는 정답일 때의 출력이 전체 값을 정한다.


```python
def cross_entropy_error(y,t):
    delta = 1e-7
    return -np.sum(t * np.log(y+delta))
```

> 아주작은 값인 delta 를 더하는 이유는 np.log 에 0을 입력하면 마이너스 무한대로 가기때문.

## 4.2.3 미니배치 학습
기계학습은 훈련데이터를 사용해 학습을 해야하는것. 훈련데이터 모두에 대한 손실 함수의 합을 구하자

빅 데이터 수준이 되면, 손실 함수의 합을 구하는데 시간이 걸린다. 따라서 근사치를 이용. (미니배치)

- 미니배치 예 (MNIST)

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

# batch size 만큼 랜덤 추출
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
# >>> np.random.choice(60000, 8)
# array([8013, 14666, 5777, 12351, 12352, 1235, 3463 ,347])

x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]

(x_batch, t_batch), (x_test, t_text) = load_mnist(normalize=True, one_hot_label=True)

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)
```

## 4.2.4 (배치용) 교차 엔트로피 오차 구현하기
```python
def cross_entropy_error(y,t):

    if y.dim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    # one_hot_label
    return -np.sum(t * np.log(y)) / batch_size
    # else
    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size
```

## 4.2.5 왜 손실 함수를 설정하는가 ?
신경망 학습에서는 최적의 매개변수(가중치, 편향)를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다. 이때 매개변수의 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.

가중치 매개변수의 손실 함수의 미분이란, '가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하냐'

> 신경망을 학습할 때 정확도를 지표로 삼아서는 안된다. 정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다.

신경망 학습에서 중요한 성질로, 기울기가 0이 되지 않는것. (시그모이드 함수)

# 4.3 수치 미분

## 4.3.1 미분

```python
# 나쁜 구현의 예
def numerical_diff(f,x):
    h = 10e-50
    return (f(x+h) -f(x)) /h
```

- h에 가능한 작은 값을 사용했으므로, 반올림 오차 문제를 야기한다.
- 함수의 차분에 대한 문제. 무한히 0으로 좁히는데는 한계가 있다.
- 전후의 차분을 계산하는 중심차분으로의 구현

```python
def numerical_diff(f,x):
    h = 1e-4 # 0.0001
    return (f(x+h) -f(x-h)) / (2*h)
```

## 4.3.2 수치 미분의 예

## 4.3.3 편미분
변수가 여럿인 함수에 대한 미분

변수중 하나의 변수에 초점을 맞추고 다른 변수의 값은 고정하여 편미분의 값을 구할 수 있다.

# 4.4 기울기
모든 변수의 편미분을 벡터로 정리한 것을 **기울기** 라고 한다.

# 작성중