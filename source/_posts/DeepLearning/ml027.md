---
title: 케라스맛 Chapter7] 케라스로 구현하는 GAN (생성적 적대 신경망)
date: 2018-05-04 17:00:52
categories:
  - DeepLearning
tags:
  - DeepLearning
  - 코딩셰프의 3분 딥러닝, 케라스맛
---

- 경쟁하여 최적화를 수행하는 생성형 신경망.
- 내부의 두 신경망이 상호 경쟁하면서 학습.
    - 하나는 생성망, 하나는 판별망

# 7.1 GAN의 원리
경쟁적 학습 방법을 이용하는 생성형 인공지능

## 7.1.1 GAN 의 목적과 개념
- 실제 데이터와 비슷한 확률분포를 가지는 허구데이터를 생성
- 실제 데이터로 얼굴 사진을 제공하면, 비슷한 확률분포를 가지는 새로운 허구 사진을 생성

- 레이블이 없는 정보를 다루는 비지도학습
- 입력데이터는 무작위 잡음. 출력데이터와 상광벗이 때문에 비지도형의 생성형 신경망이다.
- 출력데이터를 특정 분포도를 갖는 데이터로 가정. (ex_ 필기체 숫자, 사람얼굴사진)
- 잡음은 무한한 변위가 가능하므로, 학습에 사용한 어떤 데이터와도 완전히 같지 않다.

## 7.1.2 GAN의 구조
- 경쟁적인 방법으로 학습을 수행
    - 판별망은 실제 데이터인지 허구데이터인지 더 잘 구분하도록 학습
    - 생성망은 판별망을 더 잘 속이도록 학습
    - 두 과정을 계속 순환
- 목적은 학습한 실제 데이터와 같은 확률 분포를 가지는 새로운 허구 데이터를 만들도록 생성망을 학습시키는 것.

![https://www.naverlabs.com/naverlabs_/story/201712/1513772298204_%EB%8F%84%EC%8B%9D1.jpg](https://www.naverlabs.com/naverlabs_/story/201712/1513772298204_%EB%8F%84%EC%8B%9D1.jpg)
`출처 : https://www.naverlabs.com/storyDetail/44`

## 7.1.3 GAN의 동작 원리
- 생성망
    - 생성망은 저차원 무작위 잡음을 입력받아 고차원 허구이미지를 생성
    - 실제 이미지를 학습하여 실제 이미지와 확률 분포가 최대한 비슷하도록 허구 이미지를 만든다.
    - 생성망이 만든 허구이미지를 판별망이 실제 이미지로 착각하도록 만드는 방향으로 생성망 학습이 이루어진다.
- 판별망
    - 입력된 이미지가 실제인지 허구인지 판별
    - 문제는 실제 이미지는 변하지 않지만 허구 이미지는 생성망의 학습이 진행됨에 따라 점점 실제 이미지와 유사해진다는 것
    - 그래서 앞서 만들어진 이미지를 판별할 수 있도록 점진적으로 학습이 진행된다.
    - 상호 공진화 하는 방식
- 판별과 생성시 합성곱 계층을 활용하면 완전 연결 계층보다 효율적 처리 가능.

## 7.1.4 GAN의 동작 사례

![http://img1.daumcdn.net/thumb/R1920x0/?fname=http%3A%2F%2Fcfile10.uf.tistory.com%2Fimage%2F99ED60435A3BB17430B8B9](http://img1.daumcdn.net/thumb/R1920x0/?fname=http%3A%2F%2Fcfile10.uf.tistory.com%2Fimage%2F99ED60435A3BB17430B8B9)
`출처 : http://artoria.us/7`

- 판별망의 동작
    - 무작위 잡음 벡터 Z를 입력받아 생성망의 결과를 판별
    - 개별 이미지가 아닌 이미지의 확률분포를 판별
    - 실제 데이터와 생성망이 만든 허구 데이터의 확률 분포 차이를 판별하도록 학습
    
    - a. 실제 데이터를 1로 판별하는 과정    
        - 데이터로부터 가져온 샘플
            - 일반적인 배치처리.
        - 미분 가능한 판별 함수 D
        - 1을 출력하려고 노력하는 판별 함수 D
            - 정확도가 높은 판별을 위해 신경망으로 구성
    - b. 허구 데이터를 0으로 판별하는 과정
        - 무작위 잡음 벡터 Z (생성)
        - 미분 가능한 생성 함수 G
            - 복잡한 확률분포를 변환하기 위해 생성 함수 G를 신경망으로 구성
        - 모델로부터 가져온 샘플
        - 미분 가능한 판별 함수 D
        - 0을 출력하려고 노력하는 판별 함수 D

    - 판별값은 실제 데이터를 입력하면 1, 생성데이터를 입력하면 0이어야 한다.
    - 생성망의 가중치는 학습이 되지 않도록 고정.
    - 판별망에 들어가는 데이터는 실제 데이터에서 추출한 배치데이터와 생성망에서 만든 허구 데이터로 구성된다.
    - 판별망 학습 이후 생성망이 진화하기 때문에 순환적으로 계속 판별망을 학습시킨다.

- 생성망의 동작
    - 생성망의 결과가 판별망으로 들어가도록 가상 신경망 모델을 구성
    - 학습용 생성망은 새로운 신경망이 아니라 기존의 생성망과 판별망이 합쳐진 가상 신경망이다.
    - 학습용 생성망 부분에서 판별망 부분은 학습되지 않도록 가중치를 고정.
    - 판별망은 무작위 잡음 벡터 Z로부터 생성된 허구 이미지가 얼마나 실제 이미지와 유사한지 판별한 결과를 내게 된다.
    - 판별한 결과가 모두 실제가 되도록 생성망을 학습

- 진화된 생성망은 또다시 무작위 입력벡터 Z에 대해 이미지 변환을 수행.
- 이렇게 판별망과 생성망 학습이 한번씩 실행되면 GAN 전체 학습이 한번 수행된 것.
- 최적화가 완전히 끝나면 이론적으로 생성망의 결과와 실제 이미지를 판별망이 구분하지 못하게 된다.

# 7.2 확률분포 생성을 위한 완전 연결 계층 GAN 구현
- 처음 제안된 논문의 예제를 구현하자.
- 생성에 사용하는 무작위 잡음 벡터 Z는 균등분포 확률신호, 출력은 정규분포 확률 신호

## 7.2.1 패키지 임포트

```python
import numpy as np
import matplotlib.pyplot as plt

from keras import models
from keras.layers import Dense, Conv1D, Reshape, Flatten, Lambda
from keras.optimizers import Adam
from keras import backend as K


def add_decorate(x):
    """
    axis = -1 --> last dimension in an array
    """
    m = K.mean(x, axis=-1, keepdims=True)
    d = K.square(x - m)
    return K.concatenate([x, d], axis=-1)


def add_decorate_shape(input_shape):
    shape = list(input_shape)
    assert len(shape) == 2
    shape[1] *= 2
    return tuple(shape)

 model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape))


lr = 2e-4   0.0002
adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999)


def model_compile(model):
    return model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])


class GAN:
    def __init__(self, ni_D, nh_D, nh_G):
        self.ni_D = ni_D
        self.nh_D = nh_D
        self.nh_G = nh_G

        self.D = self.gen_D()
        self.G = self.gen_G()
        self.GD = self.make_GD()

    def gen_D(self):
        ni_D = self.ni_D
        nh_D = self.nh_D
        D = models.Sequential()
        D.add(Lambda(add_decorate, output_shape=add_decorate_shape, input_shape=(ni_D,)))
        D.add(Dense(nh_D, activation='relu'))
        D.add(Dense(nh_D, activation='relu'))
        D.add(Dense(1, activation='sigmoid'))

        model_compile(D)
        return D

    def gen_G(self):
        ni_D = self.ni_D
        nh_G = self.nh_D

        G = models.Sequential()   (Batch, ni_D)
        G.add(Reshape((ni_D, 1), input_shape=(ni_D,)))   (Batch, steps=ni_D, input_dim=1)
        G.add(Conv1D(nh_G, 1, activation='relu'))   (Batch, ni_D, nh_G)
        G.add(Conv1D(nh_G, 1, activation='sigmoid'))   (Batch, ni_D, nh_G)
        G.add(Conv1D(1, 1))   (Batch, ni_D, 1)
        G.add(Flatten())   (Batch, ni_D)

        model_compile(G)
        return G

    def make_GD(self):
        G, D = self.G, self.D
        GD = models.Sequential()
        GD.add(G)
        GD.add(D)
        D.trainable = False
        model_compile(GD)
        D.trainable = True
        return GD

    def D_train_on_batch(self, Real, Gen):
        D = self.D
        X = np.concatenate([Real, Gen], axis=0)
        y = np.array([1] * Real.shape[0] + [0] * Gen.shape[0])
        D.train_on_batch(X, y)

    def GD_train_on_batch(self, Z):
        GD = self.GD
        y = np.array([1] * Z.shape[0])
        GD.train_on_batch(Z, y)


class Data:
    def __init__(self, mu, sigma, ni_D):
        self.real_sample = lambda n_batch: np.random.normal(mu, sigma, (n_batch, ni_D))
        self.in_sample = lambda n_batch: np.random.rand(n_batch, ni_D)


class Machine:
    def __init__(self, n_batch=10, ni_D=100):
        data_mean = 4
        data_stddev = 1.25

        self.n_iter_D = 1
        self.n_iter_G = 5

        self.data = Data(data_mean, data_stddev, ni_D)
        self.gan = GAN(ni_D=ni_D, nh_D=50, nh_G=50)

        self.n_batch = n_batch
         self.ni_D = ni_D

    def train_D(self):
        gan = self.gan
        n_batch = self.n_batch
        data = self.data

         Real data
        Real = data.real_sample(n_batch)   (n_batch, ni_D)
         print(Real.shape)
         Generated data
        Z = data.in_sample(n_batch)   (n_batch, ni_D)
        Gen = gan.G.predict(Z)   (n_batch, ni_D)
         print(Gen.shape)

        gan.D.trainable = True
        gan.D_train_on_batch(Real, Gen)

    def train_GD(self):
        gan = self.gan
        n_batch = self.n_batch
        data = self.data
         Seed data for data generation
        Z = data.in_sample(n_batch)

        gan.D.trainable = False
        gan.GD_train_on_batch(Z)

    def train_each(self):
        for it in range(self.n_iter_D):
            self.train_D()
        for it in range(self.n_iter_G):
            self.train_GD()

    def train(self, epochs):
        for epoch in range(epochs):
            self.train_each()

    def test(self, n_test):
        """
        generate a new image
        """
        gan = self.gan
        data = self.data
        Z = data.in_sample(n_test)
        Gen = gan.G.predict(Z)
        return Gen, Z

    def show_hist(self, Real, Gen, Z):
        plt.hist(Real.reshape(-1), histtype='step', label='Real')
        plt.hist(Gen.reshape(-1), histtype='step', label='Generated')
        plt.hist(Z.reshape(-1), histtype='step', label='Input')
        plt.legend(loc=0)

    def test_and_show(self, n_test):
        data = self.data
        Gen, Z = self.test(n_test)
        Real = data.real_sample(n_test)
        self.show_hist(Real, Gen, Z)
        Machine.print_stat(Real, Gen)

    def run_epochs(self, epochs, n_test):
        """
        train GAN and show the results
        for showing, the original and the artificial results will be compared
        """
        self.train(epochs)
        self.test_and_show(n_test)

    def run(self, n_repeat=200, n_show=200, n_test=100):
        for ii in range(n_repeat):
            print('Stage', ii, '(Epoch: {})'.format(ii * n_show))
            self.run_epochs(n_show, n_test)
            plt.show()

    @staticmethod
    def print_stat(Real, Gen):
        def stat(d):
            return (np.mean(d), np.std(d))
        print('Mean and Std of Real:', stat(Real))
        print('Mean and Std of Gen:', stat(Gen))


class GAN_Pure(GAN):
    def __init__(self, ni_D, nh_D, nh_G):
        '''
        Discriminator input is not added
        '''
        super().__init__(ni_D, nh_D, nh_G)

    def gen_D(self):
        ni_D = self.ni_D
        nh_D = self.nh_D
        D = models.Sequential()
         D.add(Lambda(add_decorate, output_shape=add_decorate_shape, input_shape=(ni_D,)))
        D.add(Dense(nh_D, activation='relu', input_shape=(ni_D,)))
        D.add(Dense(nh_D, activation='relu'))
        D.add(Dense(1, activation='sigmoid'))

        model_compile(D)
        return D


class Machine_Pure(Machine):
    def __init__(self, n_batch=10, ni_D=100):
        data_mean = 4
        data_stddev = 1.25

        self.data = Data(data_mean, data_stddev, ni_D)
        self.gan = GAN_Pure(ni_D=ni_D, nh_D=50, nh_G=50)

        self.n_batch = n_batch
         self.ni_D = ni_D


def main():
    machine = Machine(n_batch=1, ni_D=100)
    machine.run(n_repeat=200, n_show=200, n_test=100)


if __name__ == '__main__':
    main()
```

```python
```