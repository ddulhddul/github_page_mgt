---
title: Chapter3. 신경망
date: 2018-01-21 19:23:52
categories:
  - DeepLearning
tags:
  - DeepLearning
  - 밑바닥부터 시작하는 딥러닝
---

![밑바닥부터 시작하는 딥러닝](/images/deeplearning/cover.jpg)

- 설치 내용
```bash
> python -m pip install --upgrade pip
> pip install six
> pip install python-dateutil
> pip install pyparsing
> pip install numpy-1.14.0+mkl-cp36-cp36m-win32.whl
> pip install matplotlib-2.1.2-cp36-cp36m-win32.whl
```

신경망은 가중치 설정을 수동으로 하던 퍼셉트론에서 벗어나 가중치 매개변수 값을 학습한다.

---
# 3.1 퍼셉트론에서 신경망으로

## 3.1.1 신경망의 예
퍼셉트론과 다르지 않다.

![신경망의 예](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)
출처 : [위키백과](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)

## 3.1.2 퍼셉트론 복습

![편향을 명시한 퍼셉트론](/images/deeplearning/perceptronWithBias.PNG)

```py 식 3.1
y = 0 (b + w1*x1 + w2*x2 <= 0)
  = 1 (b + w1*x1 + w2*x2 > 0)
```
```py 식 3.2
y = h(b + w1*x1 + w2*x2)
```
```py 식 3.3
h(x) = 0 (x <= 0)
     = 1 (x > 0)
```
> 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 **활성화 함수**(h(x)) 라고 한다.

## 3.1.3 활성화 함수의 등장
![활성화 함수의 처리 과정](/images/deeplearning/activationF.PNG)

가중치 신호를 조합한 결과가 a라는 노드가 되고, 활성화 함수 h()를 통과하여 y라는 노드로 변환

> 일반적으로 단층 퍼셉트론은 단층 네트워크에서 계단함수를 활성화 함수로 사용한 모델. 다층 퍼셉트론은 신경망(여러층 및 시그모이드 함수 등의 매끈한 활성화 함수)을 가리킨다.


---
# 3.2 활성화 함수

## 3.2.1 시그모이드 함수

```py
h(x) = 1 / (1 + exp(-x))
```

- 신경망 에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고 다음 뉴런에 전달.
- 앞장의 퍼셉트론과의 차이점은 활성화 함수 뿐.

## 3.2.2 계단 함수 구현하기
```py
def step_function(x):
    y = x > 0
    return y.astype(np.int)
```

## 3.2.3 계단 함수의 그래프
```py
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x>0, dtype=np.int)

x = np.arange(-5.0, 5.0, 0.1) # -5 ~ 5 0.1 단위
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # y축 범위 지정
plt.show()
```

![계단함수 그래프](/images/deeplearning/chapter3.01stairs.PNG)

## 3.2.4 시그모이드 함수 구현하기
```py
import numpy as np
import matplotlib.pylab as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x,y)
plt.ylim(-0.1, 1.1)
plt.show()
```

![시그모이드함수 그래프](/images/deeplearning/chapter3.02sigmoid.PNG)

## 3.2.5 시그모이드 함수와 계단 함수 비교

- 차이점
    - 매끄러움의 차이
    - 계단 함수는 0과 1중 하나의 값을 돌려주는 반면, 시그모이드 함수는 실수를 return

- 공통점
    - 입력이 중요하면 큰 값을 출력하고 중요하지 않으면 작은값을 출력
    - 입력이 아무리 작거나 커도 출력은 0과 1사이
    - 비선형 함수

## 3.2.6 비선형 함수
- 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다.
- 신경망의 층을 깊게하는 이유
- 선형 함수의 문제는 층을 아무리 깊게 해도 '은닉층이 없는 네트워크' 로도 똑같은 기능을 할 수 있다.

## 3.2.7 ReLU 함수
Rectified Linear Unit (렐루) : 정류된

```py
h(x) = x (x > 0)
     = 0 (x <= 0)
```

```py
def relu(x):
    return np.maximun(0,x)
```

> 활성화 함수로 책의 후반부에서 주로 ReLU함수 사용.

---
# 3.3 다차원 배열의 계산

## 3.3.1 다차원 배열
```py
print(A) # [1 2 3 4]
np.ndim(A) # 배열의 차원 수 : 1
A.shape # 배열의 형상 : (4,)
A.shape[0] # 4
```

## 3.3.2 행렬의 내적(행렬 곱)
```py
np.dot(A,B) # 행렬의 내적
```
- 행렬의 곱에서는 대응하는 차원의 원소 수를 일치시켜라.

## 3.3.3 신경망의 내적
다차원 배열의 내적을 구해주는 np.dot 함수를 사용해, 단번에 결과를 계산할 수 있다.

---
# 3.4 3층 신경망 구현하기

## 3.4.1 표기법 설명
이번 절에서만 사용하는 표기이므로 패스

## 3.4.2 각 층의 신호 전달 구현하기
```py
X = np.array([1.0, 0.5])

W1 = np.array([0.1, 0.3, 0.5], [0.2, 0.4, 0.6])
B1 = np.array([0.1, 0.2, 0.3])
A1 = np.dot(X, W1) + B1 # 가중치의 합
Z1 = sigmoid(A1) # 활성화 함수

W2 = np.array([0.1, 0.4], [0.2, 0.5], [0.3, 0.6])
B2 = np.array([0.1, 0.2])
A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

def identity_function(x): # 항등함수
    return x

W3 = np.array([0.1, 0.3], [0.2, 0.4])
B3 = np.array([0.1, 0.2])
A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3) # 혹은 Y = A3
```

## 3.4.3 구현 정리

---
# 3.5 출력층 설계하기
일반적으로 회귀에는 항등함수, 분류에는 소프트맥스 함수를 사용

- 분류와 회귀 ?
    - 분류 : 데이터가 어느 클래스에 속하느냐
    - 회귀 : 입력 데이터의 (연속적인) 수치를 예측



## 3.5.1 항등 함수와 소프트맥스 함수 구현하기
```py 소프트맥스 함수
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

## 3.5.2 소프트맥스 함수 구현 시 주의점
Sum 부분에 오버플로 문제를 고려해야 한다.

```py 소프트맥스 함수 (overflow 고려)
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

## 3.5.3 소프트맥스 함수의 특징
- 소프트맥스 함수 ?
    > 입력 받은 값을 0~1 사이의 값으로 모두 정규화 하며 총합은 항상 1이 되는 특성을 가진 함수

- 소프트맥스 함수의 출력을 확률로 해석할 수 있다.
- 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다.

## 3.5.4 출력층의 뉴런 수 정하기
분류에서는 분류하고 싶은 클래스 수로 설정하는것이 일반적

---
# 3.6 손글씨 숫자 인식

## 3.6.1 MNIST 데이터셋
MNIST : 손글씨 숫자 이미지 집합

```py
import sys, os
sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
from dataset.mnist import load_mnist
from PIL import Image

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

(x_train, t_train), (x_test, t_text) = load_mnist(flatten=True, normalize=False)

img = x_train[0]
label = t_train[0]
print(label)

print(img.shape)
img = img.reshape(28, 28)
print(img.shape)

img_show(img)
```

## 3.6.2 신경망의 추론 처리
- 이 신경망은 입력층 뉴런을 784(28*28)개, 출력층 뉴런을 10개로 구성
- 첫번째 은닉층에 50개의 뉴런, 두번째 은닉층에 100개의 뉴런을 배치 (임의로 정한 값)

```py
def get_data():
    (x_train, t_train), (x_test, t_test) = \
        load_mnist(normalize=True, flatten = True, one_hot_label=False)
    return x_test, t_test

def init_network():
    with open("sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(x, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(x, W3) + b3
    z3 = sigmoid(a3)

    return y
```

## 3.6.3 배치 처리

